{"task": "rl", "behavior_model": "gpt2_small_init_weight", "dataset": "politeness-0", "rl_lr": 0.0005, "batch_size": 256, "num_epoch": 20, "decoding": "stochastic", "prefix_len": 2, "gen_len": 15, "dropout": "pareto", "dropout_rate": 0.0, "reward": "r+log_b+log_p"}